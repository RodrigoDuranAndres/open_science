<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,108.43,82.34,243.92,14.93;1,108.43,102.27,321.99,14.93;1,108.43,122.19,206.33,14.93">LATENT CONSISTENCY MODELS: SYNTHESIZING HIGH-RESOLUTION IMAGES WITH FEW-STEP INFERENCE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-10-06">6 Oct 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,113.98,157.63,49.55,8.96;1,163.53,156.12,1.36,6.12"><forename type="first">Simian</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.53,157.63,42.18,8.96;1,232.71,156.12,1.36,6.12"><forename type="first">Yiqin</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.12,157.63,63.95,8.96;1,319.07,156.12,1.83,6.12"><forename type="first">Longbo</forename><surname>Huang</surname></persName>
							<email>longbohuang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,345.64,157.63,30.18,8.96;1,375.82,156.12,1.83,6.12"><forename type="first">Jian</forename><surname>Li</surname></persName>
							<email>lijian83@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,402.39,157.63,47.89,8.96;1,450.28,156.12,1.83,6.12"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
							<email>hangzhao@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,108.43,82.34,243.92,14.93;1,108.43,102.27,321.99,14.93;1,108.43,122.19,206.33,14.93">LATENT CONSISTENCY MODELS: SYNTHESIZING HIGH-RESOLUTION IMAGES WITH FEW-STEP INFERENCE</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-10-06">6 Oct 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">F6A0C454A768A26053C75D2854B5D335</idno>
					<idno type="arXiv">arXiv:2310.04378v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-02-11T17:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>latent-consistency-models</term>
					<term>github</term>
					<term>io/</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Latent Diffusion models (LDMs) have achieved remarkable results in synthesizing high-resolution images. However, the iterative sampling process is computationally intensive and leads to slow generation. Inspired by Consistency Models <ref type="bibr" coords="1,157.35,290.86,71.89,8.64" target="#b29">(Song et al., 2023)</ref>, we propose Latent Consistency Models (LCMs), enabling swift inference with minimal steps on any pre-trained LDMs, including Stable Diffusion <ref type="bibr" coords="1,184.92,312.77,91.66,8.64" target="#b20">(Rombach et al., 2022)</ref>. Viewing the guided reverse diffusion process as solving an augmented probability flow ODE (PF-ODE), LCMs are designed to directly predict the solution of such ODE in latent space, mitigating the need for numerous iterations and allowing rapid, high-fidelity sampling. Efficiently distilled from pre-trained classifier-free guided diffusion models, a high-quality 768×768 2∼4-step LCM takes only 32 A100 GPU hours for training. Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method that is tailored for fine-tuning LCMs on customized image datasets. Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-theart text-to-image generation performance with few-step inference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Diffusion models have emerged as powerful generative models that have gained significant attention and achieved remarkable results in various domains <ref type="bibr" coords="1,318.85,491.59,66.14,8.64" target="#b3">(Ho et al., 2020;</ref><ref type="bibr" coords="1,387.85,491.59,75.56,8.64">Song et al., 2020a;</ref><ref type="bibr" coords="1,466.27,491.59,37.74,8.64;1,108.00,502.55,64.78,8.64">Nichol &amp; Dhariwal, 2021;</ref><ref type="bibr" coords="1,175.40,502.55,82.03,8.64" target="#b19">Ramesh et al., 2022;</ref><ref type="bibr" coords="1,260.05,502.55,88.41,8.64" target="#b26">Song &amp; Ermon, 2019;</ref><ref type="bibr" coords="1,351.08,502.55,69.17,8.64" target="#b28">Song et al., 2021)</ref>. In particular, latent diffusion models (LDMs) (e.g., Stable Diffusion <ref type="bibr" coords="1,303.11,513.51,90.47,8.64" target="#b20">(Rombach et al., 2022)</ref>) have demonstrated exceptional performance, especially in high-resolution text-to-image synthesis tasks. LDMs can generate high-quality images conditioned on textual descriptions by utilizing an iterative reverse sampling process that performs gradual denoising of samples. However, diffusion models suffer from a notable drawback: the iterative reverse sampling process leads to slow generation speed, limiting their real-time applicability. To overcome this drawback, researchers have proposed several methods to improve the sampling speed, which involves accelerating the denoising process by enhancing ODE solvers <ref type="bibr" coords="1,139.54,590.22,67.94,8.64" target="#b3">(Ho et al., 2020;</ref><ref type="bibr" coords="1,210.95,590.22,66.10,8.64">Lu et al., 2022a;</ref><ref type="bibr" coords="1,277.05,590.22,8.42,8.64">b)</ref>, which can generate images within 10∼20 sampling steps. Another approach is to distill a pre-trained diffusion model into models that enable few-step inference <ref type="bibr" coords="1,147.95,612.14,90.27,8.64">Salimans &amp; Ho (2022)</ref>; <ref type="bibr" coords="1,245.31,612.14,73.70,8.64" target="#b14">Meng et al. (2023)</ref>. In particular, <ref type="bibr" coords="1,381.91,612.14,75.36,8.64" target="#b14">Meng et al. (2023)</ref> proposed a two-stage distillation approach to improving the sampling efficiency of classifier-free guided models. Recently, <ref type="bibr" coords="1,166.44,634.06,72.85,8.64" target="#b29">Song et al. (2023)</ref> proposed consistency models as a promising alternative aimed at speeding up the generation process. By learning consistency mappings that maintain point consistency on ODE-trajectory, these models allow for single-step generation, eliminating the need for computation-intensive iterations. However, <ref type="bibr" coords="1,285.54,666.93,73.24,8.64" target="#b29">Song et al. (2023)</ref> is constrained to pixel space image generation tasks, making it unsuitable for synthesizing high-resolution images. Moreover, the applications to the conditional diffusion model and the incorporation of classifier-free guidance have not been explored, rendering their methods unsuitable for text-to-image generation synthesis. accelerate the convergence. For 2-and 4-step inference, our method costs only 32 A100 GPU hours for training and achieves state-of-the-art performance on the LAION-5B-Aesthetics dataset.</p><p>• We introduce a new fine-tuning method for LCMs, named Latent Consistency Fine-tuning, enabling efficient adaptation of a pre-trained LCM to customized datasets while preserving the ability of fast inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Diffusion Models have achieved great success in image generation <ref type="bibr" coords="3,386.24,175.41,67.73,8.64" target="#b3">(Ho et al., 2020;</ref><ref type="bibr" coords="3,457.37,175.41,46.64,8.64;3,108.00,186.37,27.12,8.64">Song et al., 2020a;</ref><ref type="bibr" coords="3,137.23,186.37,103.38,8.64">Nichol &amp; Dhariwal, 2021;</ref><ref type="bibr" coords="3,242.72,186.37,80.52,8.64" target="#b19">Ramesh et al., 2022;</ref><ref type="bibr" coords="3,325.35,186.37,86.60,8.64" target="#b20">Rombach et al., 2022;</ref><ref type="bibr" coords="3,414.07,186.37,85.64,8.64" target="#b26">Song &amp; Ermon, 2019)</ref>. They are trained to denoise the noise-corrupted data to estimate the score of data distribution. During inference, samples are drawn by running the reverse diffusion process to gradually denoise the data point. Compared to VAEs <ref type="bibr" coords="3,218.32,219.25,109.38,8.64" target="#b7">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" coords="3,330.94,219.25,72.83,8.64" target="#b24">Sohn et al., 2015)</ref> and GANs <ref type="bibr" coords="3,453.34,219.25,50.66,8.64;3,108.00,230.21,45.41,8.64" target="#b1">(Goodfellow et al., 2020)</ref>, diffusion models enjoy the benefit of training stability and better likelihood estimation.</p><p>Accelerating DMs. However, diffusion models are bottlenecked by their slow generation speed.</p><p>Various approaches have been proposed, including training-free methods such as ODE solvers <ref type="bibr" coords="3,108.00,269.06,77.36,8.64">(Song et al., 2020a;</ref><ref type="bibr" coords="3,187.72,269.06,62.78,8.64">Lu et al., 2022a;</ref><ref type="bibr" coords="3,250.50,269.06,8.42,8.64">b)</ref>, adaptive step size solvers <ref type="bibr" coords="3,367.92,269.06,131.79,8.64" target="#b5">(Jolicoeur-Martineau et al., 2021)</ref>, predictor-corrector methods <ref type="bibr" coords="3,221.55,280.02,76.61,8.64">(Song et al., 2020b)</ref>. Training-based approaches include optimized discretization <ref type="bibr" coords="3,153.15,290.98,83.05,8.64" target="#b30">(Watson et al., 2021)</ref>, truncated diffusion <ref type="bibr" coords="3,322.73,290.98,70.18,8.64" target="#b13">(Lyu et al., 2022;</ref><ref type="bibr" coords="3,396.01,290.98,75.56,8.64" target="#b34">Zheng et al., 2022)</ref>, neural operator <ref type="bibr" coords="3,144.13,301.94,80.19,8.64" target="#b33">(Zheng et al., 2023)</ref> and distillation <ref type="bibr" coords="3,290.15,301.94,93.77,8.64">(Salimans &amp; Ho, 2022;</ref><ref type="bibr" coords="3,386.85,301.94,72.86,8.64" target="#b14">Meng et al., 2023)</ref>. More recently, new generative models for faster sampling have also been proposed <ref type="bibr" coords="3,407.83,312.90,66.69,8.64" target="#b9">(Liu et al., 2022;</ref><ref type="bibr" coords="3,477.01,312.90,21.44,8.64">2023)</ref>.</p><p>Latent Diffusion Models (LDMs) <ref type="bibr" coords="3,246.93,329.83,89.93,8.64" target="#b20">(Rombach et al., 2022)</ref> excel in synthesizing high-resolution textto-images. For example, Stable Diffusion (SD) performs forward and reverse diffusion processes in the data latent space, resulting in more efficient computation.</p><p>Consistency Models (CMs) <ref type="bibr" coords="3,223.10,368.69,73.58,8.64" target="#b29">(Song et al., 2023)</ref> have shown great potential as a new type of generative model for faster sampling while preserving generation quality. CMs adopt consistency mapping to directly map any point in ODE trajectory to its origin, enabling fast one-step generation. CMs can be trained by distilling pre-trained diffusion models or as standalone generative models. Details of CMs are elaborated in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>In this section, we briefly review diffusion and consistency models and define relevant notations.</p><p>Diffusion Models: Diffusion models, or score-based generative models <ref type="bibr" coords="3,394.78,466.46,60.93,8.64" target="#b3">Ho et al. (2020)</ref>; <ref type="bibr" coords="3,462.14,466.46,41.85,8.64;3,108.00,476.42,30.98,8.64">Song et al. (2020a)</ref> is a family of generative models that progressively inject Gaussian noises into the data, and then generate samples from noise via a reverse denoising process. In particular, diffusion models define a forward process transitioning the origin data distribution p data (x) to marginal distribution q t (x t ), via transition kernel:</p><formula xml:id="formula_0" coords="3,228.56,506.09,149.68,9.63">q0t (xt | x0) = N xt | α(t)x0, σ 2 (t)I</formula><p>, where α(t), σ(t) specify the noise schedule. In continuous time perspective, the forward process can be described by a stochastic differential equation (SDE) <ref type="bibr" coords="3,218.63,527.42,74.24,8.64">Song et al. (2020b)</ref>; <ref type="bibr" coords="3,299.44,527.42,64.33,8.64">Lu et al. (2022a)</ref>; <ref type="bibr" coords="3,370.28,527.42,76.84,8.64" target="#b6">Karras et al. (2022)</ref> for t ∈ [0, T ]:</p><formula xml:id="formula_1" coords="3,108.00,537.03,187.66,9.68">dx t = f (t)x t dt + g(t)dw t , x 0 ∼ p data (x 0 )</formula><p>, where w t is the standard Brownian motion, and</p><formula xml:id="formula_2" coords="3,197.43,554.31,306.57,21.52">f (t) = d log α(t) dt , g 2 (t) = dσ 2 (t) dt -2 d log α(t) dt σ 2 (t).<label>(1)</label></formula><p>By considering the reverse time SDE (see Appendix A for more details), one can show that the marginal distribution q t (x) satisfies the following ordinary differential equation, called the Probability Flow ODE (PF-ODE) <ref type="bibr" coords="3,221.13,602.52,78.32,8.64">(Song et al., 2020b;</ref><ref type="bibr" coords="3,301.94,602.52,64.00,8.64">Lu et al., 2022a)</ref>:</p><formula xml:id="formula_3" coords="3,200.56,619.45,303.44,19.74">dxt dt = f (t)xt - 1 2 g 2 (t)∇x log qt (xt) , xT ∼ qT (xT ) .<label>(2)</label></formula><p>In diffusion models, we train the noise prediction model ϵ θ (x t , t) to fit -∇ log q t (x t ) (called the score function). Approximating the score function by the noise prediction model in 21, one can obtain the following empirical PF-ODE for sampling:</p><formula xml:id="formula_4" coords="3,203.14,683.15,300.86,21.52">dxt dt = f (t)xt + g 2 (t) 2σt ϵ θ (xt, t) , xT ∼ N 0, σ2 I .<label>(3)</label></formula><p>For class-conditioned diffusion models, Classifier-Free Guidance (CFG) <ref type="bibr" coords="3,401.59,712.42,88.55,8.64">(Ho &amp; Salimans, 2022</ref>) is an effective technique to significantly improve the quality of generated samples and has been widely used in several large-scale diffusion models including <ref type="bibr" coords="4,323.78,85.34,108.40,8.64">GLIDE Nichol et al. (2021)</ref>, Stable Diffusion <ref type="bibr" coords="4,108.00,96.30,90.55,8.64" target="#b20">(Rombach et al., 2022)</ref>, DALL•E 2 <ref type="bibr" coords="4,250.87,96.30,86.28,8.64" target="#b19">(Ramesh et al., 2022)</ref> and Imagen <ref type="bibr" coords="4,389.65,96.30,82.25,8.64" target="#b21">(Saharia et al., 2022)</ref>. Given a CFG scale ω, the original noise prediction is replaced by a linear combination of conditional and unconditional noise prediction, i.e., εθ</p><formula xml:id="formula_5" coords="4,261.29,117.87,199.74,9.68">(z t , ω, c, t) = (1 + ω)ϵ θ (z t , c, t) -ωϵ θ (z, ∅, t).</formula><p>Consistency Models: The Consistency Model (CM) <ref type="bibr" coords="4,317.74,136.99,72.12,8.64" target="#b29">(Song et al., 2023)</ref> is a new family of generative models that enables one-step or few-step generation. The core idea of the CM is to learn the function that maps any points on a trajectory of the PF-ODE to that trajectory's origin (i.e., the solution of the PF-ODE). More formally, the consistency function is defined as f : (x t , t) -→ x ϵ , where ϵ is a fixed small positive number. One important observation is that the consistency function should satisfy the self-consistency property:</p><formula xml:id="formula_6" coords="4,238.84,203.04,265.16,10.94">f (xt, t) = f (x t ′ , t ′ ), ∀t, t ′ ∈ [ϵ, T ].<label>(4)</label></formula><p>The key idea in <ref type="bibr" coords="4,177.72,223.09,75.85,8.64" target="#b29">(Song et al., 2023)</ref> for learning a consistency model f θ is to learn a consistency function from data by effectively enforcing the self-consistency property in Eq. 4. To ensure that f θ (x, ϵ) = x, the consistency model f θ is parameterized as:</p><formula xml:id="formula_7" coords="4,234.90,261.49,269.10,8.37">f θ (x, t) = cskip(t)x + cout(t)F θ (x, t),<label>(5)</label></formula><p>where cskip(t) and cout(t) are differentiable functions with cskip(ϵ) = 1 and cout(ϵ) = 0, and F θ (x, t) is a deep neural network. A CM can be either distilled from a pre-trained diffusion model or trained from scratch. The former is known as Consistency Distillation. To enforce the self-consistency property, we maintain a target model θ -, updated with exponential moving average (EMA) of the parameter θ we intend to learn, i.e., θ -← µθ -+ (1 -µ)θ, and define the consistency loss as follows:</p><formula xml:id="formula_8" coords="4,195.19,340.09,308.81,11.61">L(θ, θ -; Φ) = Ex,t d f θ (xt n+1 , tn+1), f θ -( xϕ tn , tn) ,<label>(6)</label></formula><p>where d(•, •) is a chosen metric function for measuring the distance between two samples, e.g., the squared</p><formula xml:id="formula_9" coords="4,141.47,376.19,126.46,10.74">ℓ 2 distance d(x, y) = ||x -y|| 2 2 .</formula><p>xϕ tn is a one-step estimation of x tn from x tn+1 as:</p><formula xml:id="formula_10" coords="4,212.49,399.39,287.63,10.75">xϕ tn ← x tn+1 + (t n -t n+1 )Φ(x tn+1 , t n+1 ; ϕ). (<label>7</label></formula><formula xml:id="formula_11" coords="4,500.13,399.85,3.87,8.64">)</formula><p>where Φ denotes the one-step ODE solver applied to PF-ODE in Eq. 24. <ref type="bibr" coords="4,407.24,418.98,75.48,8.64" target="#b29">(Song et al., 2023)</ref> used Euler <ref type="bibr" coords="4,132.34,429.94,79.65,8.64">(Song et al., 2020b)</ref> or Heun solver <ref type="bibr" coords="4,277.10,429.94,80.74,8.64" target="#b6">(Karras et al., 2022)</ref> as the numerical ODE solver. More details and the pseudo-code for consistency distillation (Algorithm 2) are provided in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LATENT CONSISTENCY MODELS</head><p>Consistency Models (CMs) <ref type="bibr" coords="4,223.47,480.68,77.42,8.64" target="#b29">(Song et al., 2023)</ref> only focused on image generation tasks on Ima-geNet 64×64 <ref type="bibr" coords="4,166.78,491.64,78.49,8.64" target="#b0">(Deng et al., 2009)</ref> and LSUN 256×256 <ref type="bibr" coords="4,337.93,491.64,66.18,8.64" target="#b31">(Yu et al., 2015)</ref>. The potential of CMs to generate higher-resolution text-to-image tasks remains unexplored. In this paper, we introduce Latent Consistency Models (LCMs) in Sec 4.1 to tackle these more challenging tasks, unleashing the potential of CMs. Similar to LDMs, our LCMs adopt a consistency model in the image latent space. We choose the powerful Stable Diffusion (SD) as the underlying diffusion model to distill from. We aim to achieve few-step (2∼4) and even one-step inference on SD without compromising image quality. The classifier-free guidance (CFG) <ref type="bibr" coords="4,310.59,557.39,88.97,8.64">(Ho &amp; Salimans, 2022</ref>) is an effective technique to further improve sample quality and is widely used in SD. However, its application in CMs remains unexplored. We propose a simple one-stage guided distillation method in Sec 4.2 that solves an augmented PF-ODE, integrating CFG into LCM effectively. We propose SKIPPING-STEP technique to accelerate the convergence of LCMs in Sec. 4.3. Finally, we propose Latent Consistency Fine-tuning to finetune a pre-trained LCM for few-step inference on a customized dataset in Sec 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CONSISTENCY DISTILLATION IN THE LATENT SPACE</head><p>Utilizing image latent space in large-scale diffusion models like Stable Diffusion (SD) <ref type="bibr" coords="4,462.50,643.67,41.50,8.64;4,108.00,653.64,47.94,8.64" target="#b20">(Rombach et al., 2022)</ref> has effectively enhanced image generation quality and reduced computational load. In SD, an autoencoder (E, D) is first trained to compress high-dim image data into low-dim latent vector z = E(x), which is then decoded to reconstruct the image as x = D(z). Training diffusion models in the latent space greatly reduces the computation costs compared to pixel-based models and speeds up the inference process; LDMs make it possible to generate high-resolution images on laptop GPUs.</p><p>For LCMs, we leverage the advantage of the latent space for consistency distillation, contrasting with the pixel space used in CMs <ref type="bibr" coords="4,246.04,713.41,73.55,8.64" target="#b29">(Song et al., 2023)</ref>. This approach, termed Latent Consistency Distillation (LCD) is applied to pre-trained SD, allowing the synthesis of high-resolution (e.g., Preprint 768×768) images in 1∼4 steps. We focus on conditional generation. Recall that the PF-ODE of the reverse diffusion process <ref type="bibr" coords="5,209.44,95.30,78.32,8.64">(Song et al., 2020b;</ref><ref type="bibr" coords="5,290.25,95.30,65.57,8.64">Lu et al., 2022a)</ref> is</p><formula xml:id="formula_12" coords="5,199.90,114.29,304.10,21.52">dzt dt = f (t)zt + g 2 (t) 2σt ϵ θ (zt, c, t) , zT ∼ N 0, σ2 I ,<label>(8)</label></formula><p>where z t are image latents, ϵ θ (z t , c, t) is the noise prediction model, and c is the given condition (e.g text). Samples can be drawn by solving the PF-ODE from T to 0. To perform LCD, we introduce the consistency function f θ : (z t , c, t) → z 0 to directly predict the solution of PF-ODE (Eq. 8) for t = 0. We parameterize f θ by the noise prediction model εθ , as follows:</p><formula xml:id="formula_13" coords="5,174.96,196.69,329.04,19.88">f θ (z, c, t) = cskip(t)z + cout(t) z -σt εθ (z, c, t) αt , (ϵ-Prediction)<label>(9)</label></formula><p>where cskip(0) = 1, cout(0) = 0 and εθ (z, c, t) is a noise prediction model that initializes with the same parameters as the teacher diffusion model. Notably, f θ can be parameterized in various ways, depending on the teacher diffusion model parameterizations of predictions (e.g., x, ϵ <ref type="bibr" coords="5,461.98,250.75,42.01,8.64;5,108.00,261.70,21.44,8.64" target="#b3">(Ho et al., 2020)</ref>, v <ref type="bibr" coords="5,144.73,261.70,86.34,8.64">(Salimans &amp; Ho, 2022</ref>)). We discuss other possible parameterizations in Appendix D.</p><p>We assume that an efficient ODE solver Ψ(z t , t, s, c) is available for approximating the integration of the right-hand side of Eq equation 8 from time t to s. In practice, we can use DDIM <ref type="bibr" coords="5,480.20,287.61,23.80,8.64;5,108.00,297.57,52.03,8.64">(Song et al., 2020a)</ref>, DPM-Solver <ref type="bibr" coords="5,222.16,297.57,71.64,8.64">(Lu et al., 2022a)</ref> or DPM-Solver++ <ref type="bibr" coords="5,374.34,297.57,72.20,8.64">(Lu et al., 2022b)</ref> as</p><formula xml:id="formula_14" coords="5,461.66,297.25,42.34,8.74">Ψ(•, •, •, •).</formula><p>Note that we only use these solvers in training/distillation, not in inference. We will discuss these solvers further when we introduce the SKIPPING-STEP technique in Sec. 4.3. LCM aims to predict the solution of the PF-ODE by minimizing the consistency distillation loss <ref type="bibr" coords="5,407.58,327.46,72.32,8.64" target="#b29">(Song et al., 2023)</ref>:</p><formula xml:id="formula_15" coords="5,173.15,348.58,330.85,12.64">LCD θ, θ -; Ψ = Ez,c,n d f θ (zt n+1 , c, tn+1) , f θ -(ẑ Ψ tn , c, tn) .<label>(10)</label></formula><p>Here, ẑΨ tn is an estimation of the evolution of the PF-ODE from t n+1 → t n using ODE solver Ψ:</p><formula xml:id="formula_16" coords="5,156.76,392.70,347.24,24.27">ẑΨ tn -zt n+1 = tn t n+1 f (t)zt + g 2 (t) 2σt ϵ θ (zt, c, t) dt ≈ Ψ(zt n+1 , tn+1, tn, c),<label>(11)</label></formula><p>where the solver Ψ(•, •, •, •) is used to approximate the integration from t n+1 → t n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ONE-STAGE GUIDED DISTILLATION BY SOLVING AUGMENTED PF-ODE</head><p>Classifier-free guidance (CFG) <ref type="bibr" coords="5,233.79,459.94,93.07,8.64">(Ho &amp; Salimans, 2022)</ref> is crucial for synthesizing high-quality textaligned images in SD, typically needing a CFG scale ω over 6. Thus, integrating CFG into a distillation method becomes indispensable. Previous method Guided-Distill <ref type="bibr" coords="5,384.02,479.86,76.14,8.64" target="#b14">(Meng et al., 2023)</ref> introduces a two-stage distillation to support few-step sampling from a guided diffusion model. However, it is computationally intensive (e.g. at least 45 A100 GPUs Days for 2-step inference, estimated in <ref type="bibr" coords="5,486.85,499.79,17.16,8.64;5,108.00,509.75,45.13,8.64" target="#b10">(Liu et al., 2023)</ref>). An LCM demands merely 32 A100 GPUs Hours training for 2-step inference, as depicted in Figure <ref type="figure" coords="5,173.49,519.71,3.74,8.64">1</ref>. Furthermore, the two-stage guided distillation might result in accumulated error, leading to suboptimal performance. In contrast, LCMs adopt efficient one-stage guided distillation by solving an augmented PF-ODE. Recall the CFG used in reverse diffusion process:</p><formula xml:id="formula_17" coords="5,203.58,559.52,300.42,8.39">εθ (zt, ω, c, t) := (1 + ω)ϵ θ (zt, c, t) -ωϵ θ (zt, ∅, t) ,<label>(12)</label></formula><p>where the original noise prediction is replaced by the linear combination of conditional and unconditional noise and ω is called the guidance scale. To sample from the guided reverse process, we need to solve the following augmented PF-ODE: (i.e., augmented with the terms related to ω)</p><formula xml:id="formula_18" coords="5,194.82,617.65,305.44,21.52">dzt dt = f (t)zt + g 2 (t) 2σt εθ (zt, ω, c, t) , zT ∼ N 0, σ2 I . (<label>13</label></formula><formula xml:id="formula_19" coords="5,500.27,625.51,3.73,7.77">)</formula><p>To efficiently perform one-stage guided distillation, we introduce an augmented consistency function f θ : (z t , ω, c, t) → z 0 to directly predict the solution of augmented PF-ODE (Eq. 13) for t = 0. We parameterize the f θ in the same way as in Eq. 9, except that εθ (z, c, t) is replaced by εθ (z, ω, c, t), which is a noise prediction model initializing with the same parameters as the teacher diffusion model, but also contains additional trainable parameters for conditioning on ω. The consistency loss is the same as Eq. 10 except that we use augmented consistency function f θ (z t , ω, c, t).</p><formula xml:id="formula_20" coords="5,158.88,720.61,345.13,12.64">LCD θ, θ -; Ψ = Ez,c,ω,n d f θ (zt n+1 , ω, c, tn+1) , f θ -(ẑ Ψ,ω tn , ω, c, tn)<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>In Eq 14, ω and n are uniformly sampled from interval [ω min , ω max ] and {1, . . . , N -1} respectively. ẑΨ,ω tn is estimated using the new noise model εθ (z t , ω, c, t), as follows:</p><formula xml:id="formula_21" coords="6,109.03,119.06,409.34,75.00">ẑΨ,ω tn -zt n+1 = tn t n+1 f (t)zt + g 2 (t) 2σt εθ (zt, ω, c, t) dt = (1 + ω) tn t n+1 f (t)zt + g 2 (t) 2σt ϵ θ (zt, c, t) dt -ω tn t n+1 f (t)zt + g 2 (t) 2σt ϵ θ (zt, ∅, t) dt ≈ (1 + ω)Ψ(zt n+1 , tn+1, tn, c) -ωΨ(zt n+1 , tn+1, tn, ∅).<label>(15)</label></formula><p>Again, we can use DDIM <ref type="bibr" coords="6,218.15,196.59,77.98,8.64">(Song et al., 2020a)</ref>, DPM-Solver <ref type="bibr" coords="6,357.32,196.59,70.40,8.64">(Lu et al., 2022a)</ref> or DPM-Solver++ <ref type="bibr" coords="6,108.00,207.55,69.45,8.64">(Lu et al., 2022b)</ref> as the PF-ODE solver Ψ(•, •, •, •).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ACCELERATING DISTILLATION WITH SKIPPING TIME STEPS</head><p>Discrete diffusion models <ref type="bibr" coords="6,215.37,243.07,67.22,8.64" target="#b3">(Ho et al., 2020;</ref><ref type="bibr" coords="6,285.80,243.07,90.75,8.64" target="#b26">Song &amp; Ermon, 2019)</ref> typically train noise prediction models with a long time-step schedule {t i } i (also called discretization schedule or time schedule) to achieve high quality generation results. For instance, Stable Diffusion (SD) has a time schedule of length 1,000. However, directly applying Latent Consistency Distillation (LCD) to SD with such an extended schedule can be problematic. The model needs to sample across all 1,000 time steps, and the consistency loss attempts to aligns the prediction of LCM model f θ (zt n+1 , c, tn+1) with the prediction f θ (zt n , c, tn) at the subsequent step along the same trajectory. Since t n -t n+1 is tiny, zt n and zt n+1 (and thus f θ (zt n+1 , c, tn+1) and f θ (zt n , c, tn)) are already close to each other, incurring small consistency loss and hence leading to slow convergence. To address this issues, we introduce the SKIPPING-STEP method to considerably shorten the length of time schedule (from thousands to dozens) to achieve fast convergence while preserving generation quality.</p><p>Consistency Models (CMs) <ref type="bibr" coords="6,221.19,368.60,75.13,8.64" target="#b29">(Song et al., 2023)</ref> use the EDM <ref type="bibr" coords="6,355.54,368.60,81.21,8.64" target="#b6">(Karras et al., 2022)</ref> continuous time schedule, and the Euler, or Heun Solver as the numerical continuous PF-ODE solver. For LCMs, in order to adapt to the discrete-time schedule in Stable Diffusion, we utilize DDIM <ref type="bibr" coords="6,454.38,388.52,49.62,8.64;6,108.00,398.49,25.85,8.64">(Song et al., 2020a)</ref>, DPM-Solver <ref type="bibr" coords="6,193.54,398.49,66.47,8.64">(Lu et al., 2022a)</ref>, or DPM-Solver++ <ref type="bibr" coords="6,341.52,398.49,68.83,8.64">(Lu et al., 2022b)</ref> as the ODE solver. <ref type="bibr" coords="6,489.62,398.49,14.39,8.64;6,108.00,408.45,51.68,8.64">(Lu et al., 2022a)</ref> shows that these advanced solvers can solve the PF-ODE efficiently in Eq. 8. Now, we introduce the SKIPPING-STEP method in Latent Consistency Distillation (LCD). Instead of ensuring consistency between adjacent time steps t n+1 → t n , LCMs aim to ensure consistency between the current time step and k-step away, t n+k → t n . Note that setting k=1 reduces to the original schedule in <ref type="bibr" coords="6,118.18,448.30,71.92,8.64" target="#b29">(Song et al., 2023)</ref>, leading to slow convergence, and very large k may incur large approximation errors of the ODE solvers. In our main experiments, we set k=20, drastically reducing the length of time schedule from thousands to dozens. Results in Sec. 5.2 show the effect of various k values and reveal that the SKIPPING-STEP method is crucial in accelerating the LCD process. Specifically, consistency distillation loss in Eq. 14 is modified to ensure consistency from t n+k to t n :</p><formula xml:id="formula_22" coords="6,156.38,509.96,347.62,12.64">LCD θ, θ -; Ψ = Ez,c,ω,n d f θ (zt n+k , ω, c, t n+k ) , f θ -(ẑ Ψ,ω tn , ω, c, tn) ,<label>(16)</label></formula><p>with ẑΨ,ω tn being an estimate of z tn using numerical augmented PF-ODE solver Ψ:</p><formula xml:id="formula_23" coords="6,165.07,561.55,335.20,9.61">ẑΨ,ω tn ←-zt n+k + (1 + ω)Ψ(zt n+k , t n+k , tn, c) -ωΨ(zt n+k , t n+k , tn, ∅). (<label>17</label></formula><formula xml:id="formula_24" coords="6,500.27,561.96,3.73,7.77">)</formula><p>The above derivation is similar to Eq. 15. For LCM, we use three possible ODE solvers here: DDIM <ref type="bibr" coords="6,137.25,592.39,77.11,8.64">(Song et al., 2020a)</ref>, DPM-Solver <ref type="bibr" coords="6,274.87,592.39,67.68,8.64">(Lu et al., 2022a)</ref>, DPM-Solver++ <ref type="bibr" coords="6,414.32,592.39,68.16,8.64">(Lu et al., 2022b)</ref>, and we compare their performance in Sec 5.2. In fact, DDIM <ref type="bibr" coords="6,352.24,602.36,81.96,8.64">(Song et al., 2020a)</ref> is the first-order discretization approximation of the DPM-Solver (Proven in <ref type="bibr" coords="6,346.33,612.32,65.85,8.64">(Lu et al., 2022a)</ref>). Here we provide the detailed formula of the DDIM PF-ODE solver Ψ DDIM from t n+k to t n . The formulas of the other two solver Ψ DPM-Solver , Ψ DPM-Solver++ are provided in Appendix E.</p><formula xml:id="formula_25" coords="6,118.55,652.07,385.45,36.88">ΨDDIM(zt n+k , t n+k , tn, c) = αt n αt n+k zt n+k -σt n σt n+k • αt n αt n+k • σt n -1 εθ (zt n+k , c, t n+k ) DDIM Estimated z tn -zt n+k<label>(18)</label></formula><p>We present the pseudo-code for LCD with CFG and SKIPPING-STEP techniques in Algorithm 1</p><p>The modifications from the original Consistency Distillation (CD) algorithm in <ref type="bibr" coords="6,430.98,712.42,73.03,8.64" target="#b29">Song et al. (2023)</ref> are highlighted in blue. Also, the LCM sampling algorithm 3 is provided in Appendix B. </p><formula xml:id="formula_26" coords="7,116.97,340.80,287.58,104.94">Dz = {(z, c)|z = E(x), (x, c) ∈ D} θ -← θ repeat Sample (z, c) ∼ Dz, n ∼ U [1, N -k] and ω ∼ [ωmin, ωmax] Sample zt n+k ∼ N (α(t n+k )z; σ 2 (t n+k )I) ẑΨ,ω tn ← zt n+k + (1 + ω)Ψ(zt n+k , t n+k , tn, c) -ωΨ(zt n+k , t n+k , tn, ∅) L(θ, θ -; Ψ) ← d(f θ (zt n+k , ω, c, t n+k ), f θ -( ẑΨ,ω tn , ω, c, tn)) θ ← θ -η∇ θ L(θ, θ -) θ -← stopgrad(µθ -+ (1 -µ)θ) until convergence</formula><p>4.4 LATENT CONSISTENCY FINE-TUNING FOR CUSTOMIZED DATASET Foundation generative models like Stable Diffusion excel in diverse text-to-image generation tasks but often require fine-tuning on customized datasets to meet the requirements of downstream tasks. We propose Latent Consistency Fine-tuning (LCF), a fine-tuning method for pretrained LCM. Inspired by Consistency Training (CT) <ref type="bibr" coords="7,259.24,498.41,73.64,8.64" target="#b29">(Song et al., 2023)</ref>, LCF enables efficient few-step inference on customized datasets without relying on a teacher diffusion model trained on such data. This approach presents a viable alternative to traditional fine-tuning methods for diffusion models. The pseudo-code for LCF is provided in Algorithm 4, with a more detailed illustration in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT</head><p>In this section, we employ latency consistency distillation to train LCM on two subsets of LAION-5B. In Sec 5.1, we first evaluate the performance of LCM on text-to-image generation tasks. In Sec 5.2, we provide a detailed ablation study to test the effectiveness of using different solvers, skipping step schedules and guidance scales. Lastly, in Sec 5.3, we present the experimental results of latent consistency finetuning on a pretrained LCM on customized image datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">TEXT-TO-IMAGE GENERATION</head><p>Datasets We use two subsets of LAION-5B <ref type="bibr" coords="7,282.19,640.69,97.43,8.64" target="#b23">(Schuhmann et al., 2022)</ref>: LAION-Aesthetics-6+ (12M) and LAION-Aesthetics-6.5+ (650K) for text-to-image generation. Our experiments consider resolutions of 512×512 and 768×768. For 512 resolution, we use LAION-Aesthetics-6+, which comprises 12M text-image pairs with predicted aesthetics scores higher than 6. For 768 resolution, we use LAION-Aesthetics-6.5+, with 650K text-image pairs with aesthetics score higher than 6.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Configuration</head><p>For 512 resolution, we use the pre-trained Stable Diffusion-V2.1-Base <ref type="bibr" coords="7,477.98,701.46,20.81,8.64;7,108.00,712.42,68.63,8.64" target="#b20">(Rombach et al., 2022)</ref> as the teacher model, which was originally trained on resolution 512×512 with ϵ-Prediction <ref type="bibr" coords="7,150.97,723.38,62.33,8.64" target="#b3">(Ho et al., 2020)</ref>. For 768 resolution, we use the widely used pre-trained Stable Diffusion-  Baselines &amp; Evaluation We use DDIM <ref type="bibr" coords="8,271.00,443.58,77.08,8.64">(Song et al., 2020a)</ref>, DPM <ref type="bibr" coords="8,379.39,443.58,67.66,8.64">(Lu et al., 2022a)</ref>, DPM++ <ref type="bibr" coords="8,489.62,443.58,14.39,8.64;8,108.00,454.54,53.65,8.64">(Lu et al., 2022b)</ref> and Guided-Distill <ref type="bibr" coords="8,242.68,454.54,78.26,8.64" target="#b14">(Meng et al., 2023)</ref> as baselines. The first three are training-free samplers requiring more peak memory per step with classifier-free guidance. Guided-Distill requires two stages of guided distillation. Since Guided-Distill is not open-sourced, we strictly followed the training procedure outlined in the paper to reproduce the results. Due to the limited resource <ref type="bibr" coords="8,477.44,487.42,26.56,8.64;8,108.00,498.37,48.87,8.64" target="#b14">(Meng et al. (2023)</ref> used a large batch size of 512, requiring at least 32 A100 GPUs), we reduce the batch size to 72, the same as ours, and trained for the same 100K iterations. Reproduction details are provided in Appendix G. We admit that longer training and more computational resources can lead to better results as reported in <ref type="bibr" coords="8,229.06,531.25,75.05,8.64" target="#b14">(Meng et al., 2023)</ref>. However, LCM achieves faster convergence and superior results under the same computation cost. For evaluation, We generate 30K images from 10K text prompts in the test set (3 images per prompt), and adopt FID and CLIP scores to evaluate the diversity and quality of the generated images. We use ViT-g/14 for evaluating CLIP scores.</p><p>Results. The quantitative results in Tables <ref type="table" coords="8,283.39,581.06,4.98,8.64" target="#tab_0">1</ref> and<ref type="table" coords="8,312.00,581.06,4.98,8.64" target="#tab_1">2</ref> show that LCM notably outperforms baseline methods at 512 and 768 resolutions, especially in the low step regime (1∼4), highlighting its efficency and superior performance. Unlike DDIM, DPM, DPM++, which require more peak memory per sampling step with CFG, LCM requires only one forward pass per sampling step, saving both time and memory. Moreover, in contrast to the two-stage distillation procedure employed in Guided-Distill, LCM only needs one-stage guided distillation, which is much simpler and more practical.</p><p>The qualitative results in Figure <ref type="figure" coords="8,240.21,646.82,4.98,8.64" target="#fig_0">2</ref> further show the superiority of LCM with 2-and 4-step inference. 5.2 ABLATION STUDY ODE Solvers &amp; Skipping-Step Schedule. We compare various solvers Ψ (DDIM <ref type="bibr" coords="8,453.72,668.58,50.28,8.64;8,108.00,679.54,25.85,8.64">(Song et al., 2020a)</ref>, DPM <ref type="bibr" coords="8,166.48,679.54,69.40,8.64">(Lu et al., 2022a)</ref>, DPM++ <ref type="bibr" coords="8,279.75,679.54,70.59,8.64">(Lu et al., 2022b)</ref>) for solving the augmented PF-ODE specified in Eq 17, and explore different skipping step schedules with different k. The results are depicted in Figure <ref type="figure" coords="8,202.67,701.46,3.74,8.64" target="#fig_1">3</ref>. We observe that: 1) Using SKIPPING-STEP techniques (see Sec 4.3), LCM achieves fast convergence within 2,000 iterations in the 4-step inference setting. Specifically, the DDIM solver converges slowly at skipping step k = 1, while setting k = 5, 10, 20 leads to much faster convergence, underscoring the effectiveness of the Skipping-Step method. 2) DPM and DPM++ solvers perform better at a larger skipping step (k = 50) compared to the DDIM solver which suffers from increased ODE approximation error with larger k. This phenomenon is also discussed in <ref type="bibr" coords="9,146.61,408.20,66.04,8.64">(Lu et al., 2022a)</ref>. 3) Very small k values (1 or 5) result in slow convergence and very large ones (e.g., 50 for DDIM) may lead to inferior results. Hence, we choose k = 20, which provides competitive performance for all three solvers, for our main experiment in Sec 5.1.</p><p>The Effect of Guidance Scale ω. We examine the effect of using different CFG scales ω in LCM.</p><p>Typically, ω balances sample quality and diversity. A larger ω generally tends to improve sample quality (indicated by CLIP), but may compromise diversity (measured by FID). Beyond a certain threshold, an increased ω yields better CLIP scores at the expense of FID. Figure <ref type="figure" coords="9,447.24,479.93,4.98,8.64" target="#fig_4">4</ref> presents the results for various ω across different inference steps. Our findings include: 1) Using large ω enhances sample quality (CLIP Scores) but results in relatively inferior FID.</p><p>2) The performance gaps across 2, 4, and 8 inference steps are negligible, highlighting LCM's efficacy in 2∼8 step regions. However, a noticeable gap exists in one-step inference, indicating rooms for further improvements. We present visualizations for different ω in Figure <ref type="figure" coords="9,313.57,534.73,3.74,8.64">5</ref>. One can see clearly that a larger ω enhances sample quality, verifying the effectiveness of our one-stage guided distillation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">DOWNSTREAM CONSISTENCY FINE-TUNING RESULTS</head><p>We perform Latent Consistency Fine-tuning (LCF) on two customized image datasets, Pokemon dataset (Pinkney, 2022) and Simpsons dataset (Norod78, 2022), to demonstrate the efficiency of LCF. Each dataset, comprised of hundreds of customized text-image pairs, is split such that 90% is used for fine-tuning and the rest 10% for testing. For LCF, we utilize pretrained LCM that was originally trained at the resolution of 768×768 used in Table <ref type="table" coords="9,351.40,622.39,3.74,8.64" target="#tab_1">2</ref>. For these two datasets, we fine-tune the pre-trained LCM for 30K iterations with a learning rate 8e-6. We present qualitative results of adopting LCF on two customized image datasets in Figure <ref type="figure" coords="9,352.96,644.31,3.74,8.64" target="#fig_3">6</ref>. The finetuned LCM is capable of generating images with customized styles in few steps, showing the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We present Latent Consistency Models (LCMs), and a highly efficient one-stage guided distillation method that enables few-step or even one-step inference on pre-trained LDMs. Furthermore, we present latent consistency fine-tuning (LCF), to enable few-step inference of LCMs on customized image datasets. Extensive experiments on the LAION-5B-Aesthetics dataset demonstrate the supe- rior performance and efficiency of LCMs. Future work include extending our method to more image generation tasks such as text-guided image editing, inpainting and super-resolution.</p><p>where the s ϕ (x t , t) ≈ ∇ log q t (x t ) is a score prediction model trained via score matching <ref type="bibr" coords="13,108.00,96.30,114.52,8.64" target="#b4">(Hyvärinen &amp; Dayan, 2005;</ref><ref type="bibr" coords="13,225.93,96.30,89.51,8.64" target="#b26">Song &amp; Ermon, 2019)</ref>. Note that different noise schedules result in different PF-ODE and the PF-ODE in Eq. 24 corresponds to the EDM noise schedule <ref type="bibr" coords="13,449.89,107.26,54.12,8.64;13,108.00,118.22,21.44,8.64" target="#b6">(Karras et al., 2022)</ref>. We denote the one-step ODE solver applied to PF-ODE in Eq. 24 as Φ(x t , t; ϕ). One can either use Euler <ref type="bibr" coords="13,176.08,129.17,81.93,8.64">(Song et al., 2020b)</ref> or Heun solver <ref type="bibr" coords="13,326.14,129.17,83.02,8.64" target="#b6">(Karras et al., 2022)</ref> as the numerical ODE solver. Then, we use the ODE solver to estimate the evolution of a sample x tn from x tn+1 as:</p><formula xml:id="formula_27" coords="13,212.49,173.30,291.51,10.75">xϕ tn ← x tn+1 + (t n -t n+1 )Φ(x tn+1 , t n+1 ; ϕ).<label>(25)</label></formula><p>(Song et al., 2020b) used the same time schedule as in <ref type="bibr" coords="13,323.32,206.17,77.57,8.64" target="#b6">(Karras et al., 2022)</ref></p><formula xml:id="formula_28" coords="13,108.00,204.77,396.00,22.79">: ti = (ϵ 1/ρ + i-1 N -1 (T 1/ρ - ϵ 1/ρ</formula><p>)) ρ , and ρ = 7. To enforce the self-consistency property in Eq. 4, we maintain a target model θ -, which is updated with exponential moving average (EMA) of the parameter θ we intend to learn, i.e., θ -← µθ -+ (1 -µ)θ, and define the consistency loss as follows:</p><formula xml:id="formula_29" coords="13,195.19,273.16,308.81,11.61">L(θ, θ -; Φ) = Ex,t d f θ (xt n+1 , tn+1), f θ -( xϕ tn , tn) ,<label>(26)</label></formula><p>where d(•, •) is a chosen metric function for measuring the distance between two samples, e.g., the squared ℓ 2 distance d(x, y) = ||x -y|| 2 2 . The pseudo-code for consistency distillation in <ref type="bibr" coords="13,461.50,320.00,42.50,8.64;13,108.00,330.95,24.90,8.64" target="#b29">Song et al. (2023)</ref>. is presented in Algorithm 2. In their original paper, an Euler solver was used as the ODE solver for the continuous-time setting.</p><p>Algorithm 2 Consistency Distillation (CD) <ref type="bibr" coords="13,283.58,379.38,73.88,8.64" target="#b29">(Song et al., 2023)</ref> Input: dataset D, initial model parameter θ, learning rate η, ODE solver</p><formula xml:id="formula_30" coords="13,108.30,393.07,395.70,175.63">Φ(•, •, •), distance metric d(•, •), and EMA rate µ θ -← θ repeat Sample x ∼ D and n ∼ U[1, N -1] Sample x tn+1 ∼ N (x; t 2 n+1 I) xϕ tn ← x tn+1 + (t n -t n+1 )Φ(x tn+1 , t n+1 , ϕ) L(θ,θ -; Φ) ← d(f θ (x tn+1 , t n+1 ), f θ -( xϕ tn , t n )) θ ← θ -η∇ θ L(θ, θ -; Φ) θ -← stopgrad(µθ -+ (1 -µ)θ) until convergence B MULTISTEP LATENT CONSISTENCY SAMPLING</formula><p>Now, we present the multi-step sampling algorithm for latent consistency model. The sampling algorithm for LCM is very similar to the one in consistency models <ref type="bibr" coords="13,384.69,602.83,75.34,8.64" target="#b29">(Song et al., 2023)</ref> except the incorporation of classifier-free guidance in LCM. Unlike multi-step sampling in diffusion models, in which we predict z t-1 from z t , the latent consistency models directly predicts the origin z 0 of augmented PF-ODE trajectory (the solution of the augmented of PF-ODE), given guidance scale ω. This generates samples in a single step. The sample quality can be improved by alternating the denoising and noise injection steps. In particular, in the n-th iteration, we first perform noiseinjecting forward process to the previous predicted sample z as ẑτn ∼ N (α(τ n )z; σ 2 (τ n )I), where τ n is a decreasing sequence of time steps. This corresponds to going back to point ẑτn on the PF-ODE trajectory. Then, we perform the next z 0 prediction again using the trained latent consistency function. In our experiments, one can see the second iteration can already refine the generation quality significantly, and high quality images can be generated in just 2-4 steps. We provide the pseudo-code in Algorithm 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Multistep Latent Consistency Sampling</head><p>Input:</p><formula xml:id="formula_31" coords="14,117.96,98.69,386.04,107.55">Latent Consistency Model f θ (•, •, •, •), Sequence of timesteps τ 1 &gt; τ 2 &gt; • • • &gt; τ N -1 , Text condition c, Classifier-Free Guidance Scale ω, Noise schedule α(t), σ(t), Decoder D(•) Sample initial noise ẑT ∼ N (0; I) z ← f θ ( ẑT , ω, c, T ) for n = 1 to N -1 do ẑτn ∼ N (α(τ n )z; σ 2 (τ n )I) z ← f θ ( ẑτn , ω, c, τ n ) end for x ← D(z) Output: x</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ALGORITHM DETAILS OF LATENT CONSISTENCY FINE-TUNING</head><p>In this section, we provide further details of Latent Consistency Fine-tuning (LCF). The pseudo-code of LCF is provided in Algorithm 4. During the Latent Consistency Fine-tuning (LCF) process, we randomly select two time steps t n and t n+k that are k time steps apart and apply the same Gaussian noise ϵ to obtain the noised data z tn , z t n+k as follows:</p><formula xml:id="formula_32" coords="14,181.28,302.29,249.43,10.35">z t n+k = α(t n+k )z + σ(t n+k )ϵ , z tn = α(t n )z + σ(t n )ϵ.</formula><p>Then, we can directly calculate the consistency loss for these two time steps to enforce selfconsistency property in Eq.4. Notably, this method can also utilize the skipping-step technique to speedup the convergence. Furthermore, we note that latent consistency fine-tuning is independent of the pre-trained teacher model, facilitating direct fine-tuning of a pre-trained latent consistency model without reliance on the teacher diffusion model. </p><formula xml:id="formula_33" coords="14,117.96,431.32,338.30,114.83">z = {(z, c)|z = E(x), (x, c) ∈ D (s) } θ -← θ repeat Sample (z, c) ∼ D (s) z , n ∼ U[1, N -k] and w ∼ [w min , w max ] Sample ϵ ∼ N (0, I) z t n+k ← α(t n+k )z + σ(t n+k )ϵ , z tn ← α(t n )z + σ(t n )ϵ L(θ, θ -) ← d(f θ (z t n+k , t n+k , c, w), f θ -(z tn , t n , c, w)) θ ← θ -η∇ θ L(θ, θ -) θ -← stopgrad(µθ -+ (1 -µ)θ) until convergence</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DIFFERENT WAYS TO PARAMETERIZE THE CONSISTENCY FUNCTION</head><p>As previously discussed in Eq 9, we can parameterize our consistency model function f θ (z, c, t) in different ways, depending on the way the teacher diffusion model is parameterized. For ϵ-Prediction <ref type="bibr" coords="14,159.58,617.40,76.49,8.64">(Song et al., 2020a)</ref>, we use the following parameterization:</p><formula xml:id="formula_34" coords="14,108.00,631.20,396.00,45.55">f θ (z, c, t) = c skip (t)z + c out (t) ẑ0 (ϵ-Prediction) (27) where ẑ0 = z t -σ(t)ε θ (z, c, t) α(t) . (<label>28</label></formula><formula xml:id="formula_35" coords="14,499.85,661.50,4.15,8.64">)</formula><p>Recalling that z t = α(t)z 0 + σ(t)ϵ, ẑ0 can be seen as a prediction of z 0 at time t.</p><p>Next, we provide the parameterization of (x-Prediction) <ref type="bibr" coords="14,341.04,698.15,67.51,8.64" target="#b3">(Ho et al., 2020;</ref><ref type="bibr" coords="14,411.86,698.15,92.14,8.64">Salimans &amp; Ho, 2022)</ref> with the following form:</p><formula xml:id="formula_36" coords="14,183.64,723.03,320.36,9.84">f θ (z, c, t) = c skip (t)z + c out (t)x θ (z t , c, t), (x-Prediction)<label>(29)</label></formula><p>where x θ (z t , c, t) corresponds to the teacher diffusion model with x-prediction.</p><p>Finally, for v-prediction <ref type="bibr" coords="15,206.90,102.28,91.18,8.64">(Salimans &amp; Ho, 2022)</ref>, the consistency function is parameterized as</p><formula xml:id="formula_37" coords="15,158.69,118.83,345.31,9.84">f θ (z, c, t) = c skip (t)z + c out (t) (α t z t -σ t v θ (z t , c, t)) , (v-Prediction)<label>(30)</label></formula><p>where v θ (z t , c, t) corresponds to the teacher diffusion model with v-prediction.</p><p>As mentioned in Sec 5.1, we use the ϵ-Parameterization in Eq. 27 to train LCM at 512×512 resolution using the teacher diffusion model, Stable-Diffusion-V2.1-Base (originally trained with ϵ-Prediction at 512 resolution). For resolution 768×768, we train the LCM using the v-Parameterization in Eq. 30, adopting the teacher diffusion model, Stable-Diffusion-V2.1 (originally trained with v-Prediction at 768 resolution).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E FORMULAS OF OTHER ODE SOLVERS</head><p>As discussed in Sec 4.3, we use the DDIM <ref type="bibr" coords="15,289.99,249.05,79.61,8.64">(Song et al., 2020a)</ref>, DPM-Solver <ref type="bibr" coords="15,431.99,249.05,72.01,8.64">(Lu et al., 2022a)</ref> and DPM-Solver++ <ref type="bibr" coords="15,191.52,260.01,72.66,8.64">(Lu et al., 2022b)</ref> as the PF-ODE solvers. Proven in <ref type="bibr" coords="15,413.41,260.01,70.28,8.64">(Lu et al., 2022a)</ref>, the DDIM-Solver is actually the first-order discretization approximation of the DPM-Solver.</p><p>For DDIM <ref type="bibr" coords="15,155.14,287.90,79.51,8.64">(Song et al., 2020a)</ref> , the detailed formula of DDIM PF-ODE solver Ψ DDIM from t n+k to t n is provided as follows.</p><formula xml:id="formula_38" coords="15,115.31,315.18,388.69,65.49">Ψ DDIM (z t n+k , t n+k , t n , c) = ẑtn -z t n+k = α tn α t n+k z t n+k -σ tn σ t n+k • α tn α t n+k • σ tn -1 εθ (z t n+k , c, t n+k ) DDIM Estimated zt n -z t n+k<label>(31)</label></formula><p>For DPM-Solver <ref type="bibr" coords="15,181.87,388.97,70.35,8.64">(Lu et al., 2022a)</ref>, we only consider the case for order = 2, and the detailed formula of PF-ODE solver Ψ DPM-Solver is provided as follows. First we define some notations. We denote λ tn = log(</p><formula xml:id="formula_39" coords="15,186.60,407.53,12.37,16.24">αt n σt n</formula><p>), which is the Log-SNR, h 0 tn = λ tn -λ t n+k , h </p><p>where ε is the noise prediction model, and z Ψ t n+k/2 is the middle point between n + k and n, given by the following formula:</p><formula xml:id="formula_41" coords="15,172.01,538.10,331.99,25.11">z Ψ t n+k/2 = α t n+k/2 α t n+k z t n+k -σ t n+k/2 (e h 1 tn -1)ε θ (z t n+k , c, t n+k )<label>(33)</label></formula><p>For DPM-Solver++ <ref type="bibr" coords="15,193.00,576.44,70.47,8.64">(Lu et al., 2022b)</ref>, we consider the case for order = 2, DPM-Solver++ replaces the original noise prediction to data prediction <ref type="bibr" coords="15,325.81,587.40,68.93,8.64">(Lu et al., 2022b)</ref>, with the detailed formula of Ψ DPM-Solver++ provided as follows.</p><formula xml:id="formula_42" coords="15,150.64,614.78,353.36,60.57">Ψ DPM-Solver++ (z t n+k , t n+k , t n , c) = σ tn σ t n+k z t n+k -α tn (e -h 0 tn -1) xθ (z t n+k , c, t n+k ) - α tn 2r tn (e -h 0 tn -1) xθ (z Ψ t n+k/2 , c, t n+k/2 ) -xθ (z t n+k , c, t n+k ) -z t n+k ,<label>(34)</label></formula><p>where x is the data prediction model <ref type="bibr" coords="15,263.69,683.83,71.73,8.64">(Lu et al., 2022a)</ref> and z Ψ t n+k/2 is the middle point between n + k and n, given by the following formula:</p><formula xml:id="formula_43" coords="15,168.03,709.86,335.97,25.12">z Ψ t n+k/2 = σ t n+k/2 σ t n+k z t n+k -α t n+k/2 (e -h 1 tn -1) xθ (z t n+k , c, t n+k )<label>(35)</label></formula><p>F TRAINING DETAILS OF LATENT CONSISTENCY DISTILLATION As mentioned in Section 5.1, we conduct our experiments in two resolution settings 512×512 and 768×768. For the former setting, we use the LAION-Aesthetics-6+ <ref type="bibr" coords="16,381.65,119.74,100.89,8.64" target="#b23">(Schuhmann et al., 2022)</ref> 12M dataset, consisting of 12M text-image pairs with predicted aesthetics scores higher than 6. For the latter setting, we use the LAIOIN-Aesthetic-6.5+ <ref type="bibr" coords="16,308.39,141.66,99.70,8.64" target="#b23">(Schuhmann et al., 2022)</ref>, which comprise 650K text-image pairs with predicted aesthetics scores higher than 6.5.</p><p>For 512×512 resolution, we train the LCM with the teacher diffusion model Stable-Diffusion-V2.1-Base (SD-V2.1-Base) <ref type="bibr" coords="16,199.04,180.51,92.31,8.64" target="#b20">(Rombach et al., 2022)</ref>, which is originally trained on 512×512 resolution images using the ϵ-Prediction <ref type="bibr" coords="16,232.74,191.47,66.54,8.64" target="#b3">(Ho et al., 2020)</ref>. We train LCM (512×512) with 100K iterations on 8 A100 GPUs, using a batch size of 72, the same learning rate 8e-6 , EMA rate µ = 0.999943 and Rectified Adam optimizer <ref type="bibr" coords="16,232.48,213.39,68.45,8.64" target="#b8">(Liu et al., 2019)</ref> used in <ref type="bibr" coords="16,335.65,213.39,73.30,8.64" target="#b29">(Song et al., 2023)</ref>. We select the DDIM-Solver <ref type="bibr" coords="16,137.01,224.35,80.26,8.64">(Song et al., 2020a)</ref> and skipping step k = 20 in Eq. 17. We set the guidance scale range [ω min , ω max ] = [2, 14], which is consistent with the setting in Guided-Distill <ref type="bibr" coords="16,422.06,235.31,77.65,8.64" target="#b14">(Meng et al., 2023)</ref>.</p><p>During training, we initialize the consistency function f θ (z tn , ω, c, t n ) with the same parameters as the teacher diffusion model (SD-V2.1-Base). To encode the CFG scale ω into the LCM, we applying Fourier embedding to ω, integrating it into the origin LCM backbone by adding the projected ω-embedding into the original embedding, as done in <ref type="bibr" coords="16,328.72,279.14,76.95,8.64" target="#b14">(Meng et al., 2023)</ref>. We use a zero parameter initialization method mentioned in <ref type="bibr" coords="16,268.62,290.10,110.10,8.64" target="#b32">(Zhang &amp; Agrawala, 2023)</ref> on projected ω-embedding for better training stability. For training LCM (512×512), we use a augmented consistency function parameterized in ϵ-prediction as discussed in Appendix. D.</p><p>For 768×768 resolution, we train the LCM with the teacher diffusion model Stable-Diffusion-V2.1 (SD-V2.1) <ref type="bibr" coords="16,152.82,339.91,89.80,8.64" target="#b20">(Rombach et al., 2022)</ref>, which is originally trained on 768×768 resolution images using the v-Prediction <ref type="bibr" coords="16,174.94,350.87,90.44,8.64">(Salimans &amp; Ho, 2022)</ref>. We train LCM (768×768) with 100K iterations on 8 A100 GPUs using a batch size of 16, while the other hyper-parameters remain the same as in 512×512 resolution setting.</p><p>G REPRODUCTION DETAILS OF GUIDED-DISTILL Guided-Distill <ref type="bibr" coords="16,168.52,424.90,78.04,8.64" target="#b14">(Meng et al., 2023)</ref> serves as a significant baseline for guided distillation but is not open-sourced. We adhered strictly to the training procedure described in the paper, reproducing the method for accurate comparisons. For 512×512 resolution setting, Guided-Distill <ref type="bibr" coords="16,452.80,446.82,51.20,8.64;16,108.00,457.78,23.24,8.64" target="#b14">(Meng et al., 2023)</ref> used a large batch size of 512, which requires at least 32 A100 GPUs for training. Due to limited resource, we reduced the batch size to 72 (512 resolution), while set the batchsize to 16 for 768 resolution, the same as ours, and trained for 100K iterations, also the same as in LCM.</p><p>Specifically, Guided Distill involves two stages of distillation. For the first stage, it use a student model to fit the outputs of the pre-trained guided diffusion model using classifier-free guidance scales ω. The loss function is as follows:</p><formula xml:id="formula_44" coords="16,188.00,532.87,316.00,12.69">E w∼pw,t∼U [0,1],x∼pdata (x)[ω(λ t )|| xη1 (z t , w) -xw θ (z t )|| 2 2 ],<label>(36)</label></formula><p>where xθ (z t ) = (1 + w) xc,θ (z t ) -w xθ (z t ), z t ∼ q(z t |x) and p w (w) = U[w min , w max ].</p><p>In our implementation, we follow the same training procedure in <ref type="bibr" coords="16,379.03,569.95,79.83,8.64" target="#b14">(Meng et al., 2023)</ref> except the difference of computation resources. For first stage distillation, we train the student model with 25,000 gradient updates (batch size 72), roughly the same computation costs in <ref type="bibr" coords="16,427.02,591.87,76.98,8.64" target="#b14">(Meng et al., 2023)</ref> (3,000 gradient updates, batch size 512), and we reduce the original learning rate 1e -4 to 5e -5 for smaller batch size. For second stage distillation, we progressively train the student model using the same schedule as in Guided-Distill <ref type="bibr" coords="16,270.26,624.75,79.74,8.64" target="#b14">(Meng et al., 2023)</ref> except for batch size difference. We train the student model with 2500 gradient updates except when the sampling step equals to 1,2, or 4, where we train for 20000 gradient updates, the same schedule used in <ref type="bibr" coords="16,405.34,646.66,76.55,8.64" target="#b14">(Meng et al., 2023)</ref>. We trained until the total number of gradient iterations for the entire stage reached 100K the same as LCM training. The generation results of Guided Distill are shown in Figure <ref type="figure" coords="16,412.39,668.58,3.74,8.64" target="#fig_0">2</ref>. We can also see that the performances in Table <ref type="table" coords="16,213.29,679.54,4.98,8.64" target="#tab_0">1</ref> and Table <ref type="table" coords="16,261.50,679.54,4.98,8.64" target="#tab_1">2</ref> are similar, further verifying the correctness of our Guided-Distill implementation. Nevertheless, we acknowledge that longer training and more computational resources can lead to better results as reported in <ref type="bibr" coords="16,300.99,701.46,73.79,8.64" target="#b14">(Meng et al., 2023)</ref>. However, LCM achieves faster convergence and superior results under the same computation cost (same batch size, same number of iterations), demonstrating its practicability and superiority.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,108.00,206.18,396.00,8.64;8,108.00,216.79,356.57,7.77;8,98.22,224.08,152.71,109.08"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Text-to-Image generation results on LAION-Aesthetic-6.5+ with 2-, 4-step inference. Images generated by LCM exhibit superior detail and quality, outperforming other baselines by a large margin.</figDesc><graphic coords="8,98.22,224.08,152.71,109.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,108.00,338.96,396.00,8.64;8,108.00,349.57,361.17,7.77;8,108.00,371.50,396.00,8.99;8,108.00,382.49,396.00,8.96;8,108.00,393.45,396.00,8.96;8,108.00,404.41,396.00,8.96;8,108.00,415.37,396.00,9.81;8,108.00,426.64,357.96,8.64;8,228.81,224.08,152.71,109.08"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Ablation study on different ODE solvers and skipping step k. Appropriate skipping step k can significantly accelerate convergence and lead to better FID within the same number of training steps.V2.1, originally trained on resolution 768×768 with v-Prediction(Salimans &amp; Ho, 2022). We train LCM with 100K iterations and we use a batch size of 72 for (512 × 512) setting, and 16 for (768 × 768) setting, the same learning rate 8e-6 and EMA rate µ = 0.999943 as used in<ref type="bibr" coords="8,456.42,393.77,47.58,8.64;8,108.00,404.73,21.44,8.64" target="#b29">(Song et al., 2023)</ref>. For augmented PF-ODE solver Ψ and skipping step k in Eq. 17, we use DDIM-Solver(Song  et al., 2020a)  with skipping step k = 20. We set the guidance scale range [w min , w max ] = [2, 14], consistent with<ref type="bibr" coords="8,170.55,426.64,74.84,8.64" target="#b14">(Meng et al., 2023)</ref>. More training details are provided in the Appendix F.</figDesc><graphic coords="8,228.81,224.08,152.71,109.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,108.00,178.55,396.00,8.64;9,108.00,189.16,378.42,7.77;9,146.30,204.18,17.41,4.98;9,188.67,204.18,17.41,4.98;9,231.05,204.18,17.41,4.98;9,272.01,204.18,20.21,4.98;9,318.43,204.18,17.41,4.98;9,360.80,204.18,17.41,4.98;9,403.18,204.18,17.41,4.98;9,444.14,204.18,20.21,4.98;9,150.54,78.41,155.81,97.38"><head>Figure 4 :Figure 5</head><label>45</label><figDesc>Figure 4: Ablation study on different classifier-free guidance scales ω. Larger ω leads to better sample quality (CLIP Scores). The performance gaps across 2, 4, and 8 steps are minimal, showing the efficacy of LCM. 𝛚 = 2.0 𝛚 = 4.0 𝛚 = 8.0 𝛚 = 12.0 𝛚 = 2.0 𝛚 = 4.0 𝛚 = 8.0 𝛚 = 12.0</figDesc><graphic coords="9,150.54,78.41,155.81,97.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,108.00,222.54,396.00,8.64;10,108.00,233.15,366.91,7.77"><head>Figure 6</head><label>6</label><figDesc>Figure 6: 4-step LCMs using Latent Consistency Fine-tuning (LCF) on two customized datasets: Pokemon Dataset (left), Simpsons Dataset (right). Through LCF, LCM produces images with customized styles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="14,108.00,381.63,207.12,9.03;14,117.96,397.02,118.09,9.03;14,236.33,395.51,9.99,6.12;14,246.82,397.06,257.18,8.99;14,117.96,408.04,386.04,9.81;14,117.96,419.00,69.34,8.96;14,117.96,432.90,177.89,8.96;14,296.13,429.76,9.99,6.12"><head>Algorithm 4</head><label>4</label><figDesc>Latent Consistency Fine-tuning (LCF) Input: customized dataset D (s) , pre-trained LCM parameter θ, learning rate η, distance metric d(•, •), EMA rate µ, noise schedule α(t), σ(t), guidance scale [w min , w max ], skipping interval k, and encoder E(•) Encode training data into the latent space: D (s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,108.00,86.42,396.00,181.25"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results with ω = 8 at 512×512 resolution. LCM significantly surpasses baselines in the 1-4 step region on LAION-Aesthetic-6+ dataset. For LCM, DDIM-Solver is used with a skipping step of k = 20.</figDesc><table coords="7,111.96,86.42,385.61,181.25"><row><cell>MODEL (512 × 512) RESO</cell><cell cols="3">FID ↓ 1 STEP 2 STEPS 4 STEPS 8 STEPS 1 STEPS 2 STEPS 4 STEPS 8 STEPS CLIP SCORE ↑</cell></row><row><cell>DDIM (Song et al., 2020a)</cell><cell>183.29 81.05 22.38 13.83</cell><cell>6.03</cell><cell>14.13 25.89 29.29</cell></row><row><cell>DPM (Lu et al., 2022a)</cell><cell>185.78 72.81 18.53 12.24</cell><cell>6.35</cell><cell>15.10 26.64 29.54</cell></row><row><cell>DPM++ (Lu et al., 2022b)</cell><cell>185.78 72.81 18.43 12.20</cell><cell>6.35</cell><cell>15.10 26.64 29.55</cell></row><row><cell cols="2">Guided-Distill (Meng et al., 2023) 108.21 33.25 15.12 13.89</cell><cell cols="2">12.08 22.71 27.25 28.17</cell></row><row><cell>LCM (Ours)</cell><cell>35.36 13.31 11.10 11.84</cell><cell cols="2">24.14 27.83 28.69 28.84</cell></row><row><cell>MODEL (768 × 768) RESO</cell><cell cols="3">FID ↓ 1 STEP 2 STEPS 4 STEPS 8 STEPS 1 STEPS 2 STEPS 4 STEPS 8 STEPS CLIP SCORE ↑</cell></row><row><cell>DDIM (Song et al., 2020a)</cell><cell>186.83 77.26 24.28 15.66</cell><cell>6.93</cell><cell>16.32 26.48 29.49</cell></row><row><cell>DPM (Lu et al., 2022a)</cell><cell>188.92 67.14 20.11 14.08</cell><cell>7.40</cell><cell>17.11 27.25 29.80</cell></row><row><cell>DPM++ (Lu et al., 2022b)</cell><cell>188.91 67.14 20.08 14.11</cell><cell>7.41</cell><cell>17.11 27.26 29.84</cell></row><row><cell cols="2">Guided-Distill (Meng et al., 2023) 120.28 30.70 16.70 14.12</cell><cell cols="2">12.88 24.88 28.45 29.16</cell></row><row><cell>LCM (Ours)</cell><cell>34.22 16.32 13.53 14.97</cell><cell cols="2">25.32 27.92 28.60 28.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,108.00,272.50,396.00,76.38"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results with ω = 8 at 768×768 resolution. LCM significantly surpasses the baselines in the 1-4 step region on LAION-Aesthetic-6.5+ dataset. For LCM, DDIM-Solver is used with a skipping step of k = 20.</figDesc><table coords="7,108.00,306.88,207.12,9.03"><row><cell>Algorithm 1 Latent Consistency Distillation (LCD)</cell></row></table><note coords="7,116.97,320.83,387.03,8.12;7,116.97,330.86,380.92,8.06;7,116.97,341.11,144.69,7.77"><p><p>Input: dataset D, initial model parameter θ, learning rate η, ODE solver Ψ(•, •, •, •), distance metric d(•, •), EMA rate µ,</p>noise schedule α(t), σ(t), guidance scale [wmin, wmax], skipping interval k, and encoder E(•) Encoding training data into latent space:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="15,108.00,408.99,396.00,94.72"><head></head><label></label><figDesc>1 tn = λ tn -λ t n+k/2 , and r tn = h 1 tn /h 0 tn . Ψ DPM-Solver (z t n+k , t n+k , t n , c)</figDesc><table coords="15,157.72,454.83,46.79,48.88"><row><cell>= z t -α tn α t n+k σ tn 2r tn (e h 0</cell></row></table><note coords="15,203.46,467.53,13.28,4.37;15,220.04,461.57,32.56,9.65;15,252.60,457.87,8.05,7.75;15,257.26,461.57,64.74,10.32;15,323.08,461.54,37.39,9.68;15,201.12,487.09,56.06,8.88;15,257.60,485.16,6.12,6.12;15,257.18,491.73,23.43,7.06;15,281.61,487.09,92.64,10.46;15,375.32,487.20,77.34,10.35;15,453.73,487.23,2.77,8.74"><p>n+k -σ tn (e h 0 tn -1)ε θ (z t n+k , c, t n+k ) tn -1) εθ (z Ψ t n+k/2 , c, t n+k/2 ) -εθ (z t n+k , c, t n+k ) -z t n+k ,</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A MORE DETAILS ON DIFFUSION AND CONSISTENCY MODELS</head><p>A.1 DIFFUSION MODELS Consider the forward process, described by the following SDE for t ∈ [0, T ]:</p><p>where w t denotes the standard Brownian motion. Leveraging the classic result of <ref type="bibr" coords="12,434.00,348.57,65.86,8.64">Anderson (1982)</ref>, <ref type="bibr" coords="12,108.00,358.53,78.23,8.64">Song et al. (2020b)</ref> show that the reverse process of the above forward process is also a diffusion process, specified by the following reverse-time SDE:</p><p>where w t is a standard reverse-time Brownian motion. One can leverage the reverse SDE for data sampling from T to 0, starting with q T (x T ), which follows a Gaussian distribution approximately. However, directly sampling from the reverse SDE requires a large number of discretization steps and is typically very slow. To accelerate the sampling process, prior work (e.g., <ref type="bibr" coords="12,425.74,433.54,78.27,8.64">(Song et al., 2020b;</ref><ref type="bibr" coords="12,108.00,443.51,64.87,8.64">Lu et al., 2022a)</ref> leveraged the relation between the above SDE and ODE and designed ODE solvers for sampling. In particular, it is known that for SDE (Eq.20), the following ordinary differential equation (ODE), called the Probability Flow ODE (PF-ODE), has the same marginal distribution q t (x) <ref type="bibr" coords="12,132.76,473.39,78.32,8.64">(Song et al., 2020b;</ref><ref type="bibr" coords="12,213.57,473.39,64.00,8.64">Lu et al., 2022a)</ref>:</p><p>The term -∇ log q t (x t ) in Eq. 21 is typically called the score function of q t (x t ). In diffusion models, we train the noise prediction model ϵ θ (x t , t) to fit the scaled score function, via minimizing the following score matching objective:</p><p>where w(t) is the weight function, ϵ ∼ N (0, I) and x t = α(t)x 0 + σ(t)ϵ. By substituting the score function with the noise prediction model in Eq. 21, we obtain the following ODE, which can be used for sampling: In this subsection, we provide more details on the consistency models and consistency distillation algorithm in <ref type="bibr" coords="12,159.69,687.17,72.40,8.64" target="#b29">(Song et al., 2023)</ref>. The pre-trained diffusion model used in <ref type="bibr" coords="12,401.20,687.17,74.19,8.64" target="#b29">(Song et al., 2023)</ref> adopts the continuous noise schedule from EDM <ref type="bibr" coords="12,276.16,697.14,78.11,8.64" target="#b6">(Karras et al., 2022)</ref>, therefore the PF-ODE in Eq. 23 can be simplified as:</p><p>Preprint </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4-Steps Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H MORE FEW-STEP INFERENCE RESULTS</head><p>We present more images (768×768) generation results with LCM using 4 and 2-steps inference in Figure <ref type="figure" coords="17,136.26,647.42,4.98,8.64">7</ref> and Figure <ref type="figure" coords="17,188.40,647.42,3.74,8.64">8</ref>. It is evident that LCM is capable of synthesizing high-resolution images with just 2, 4 steps of inference. Moreover, LCM can be derived from any pre-trained Stable Diffusion (SD) <ref type="bibr" coords="17,130.87,669.34,94.62,8.64" target="#b20">(Rombach et al., 2022)</ref> in merely 4,000 training steps, equivalent to around 32 A100 GPU Hours, showcasing the effectiveness and superiority of LCM.</p><p>Preprint </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-Steps Inference</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,108.00,306.92,396.00,8.64;10,117.96,317.70,386.04,8.82;10,117.96,328.84,98.78,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,394.53,306.92,109.47,8.64;10,117.96,317.88,101.07,8.64">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,259.41,317.70,240.59,8.59">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,347.77,396.00,8.64;10,117.96,358.55,386.04,8.82;10,117.96,369.51,117.31,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,274.43,358.73,127.21,8.64">Generative adversarial networks</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,410.79,358.55,93.21,8.59;10,117.96,369.51,17.42,8.59">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,388.62,147.90,8.64;10,276.04,388.62,145.74,8.64;10,441.91,388.44,62.09,8.59;10,117.96,399.40,100.17,8.82" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,276.04,388.62,141.47,8.64">Classifier-free diffusion guidance</title>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.12598</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,108.00,418.33,396.00,8.82;10,117.96,429.29,247.91,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,285.33,418.51,161.36,8.64">Denoising diffusion probabilistic models</title>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,455.38,418.33,48.62,8.59;10,117.96,429.29,156.20,8.59">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,448.40,396.00,8.64;10,117.96,459.18,224.04,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,247.78,448.40,256.22,8.64;10,117.96,459.36,11.42,8.64">Estimation of non-normalized statistical models by score matching</title>
		<author>
			<persName coords=""><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,136.77,459.18,154.40,8.59">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,478.29,396.00,8.64;10,117.96,489.07,386.04,8.82;10,117.96,500.20,22.42,8.64" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,117.96,489.25,241.25,8.64">Gotta go fast when generating data with score-based models</title>
		<author>
			<persName coords=""><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rémi</forename><surname>Piché-Taillefer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tal</forename><surname>Kachman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14080</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,108.00,519.13,396.00,8.64;10,117.96,529.91,386.03,8.82;10,117.96,541.05,22.42,8.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,338.91,519.13,165.09,8.64;10,117.96,530.09,95.28,8.64">Elucidating the design space of diffusionbased generative models</title>
		<author>
			<persName coords=""><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,220.97,529.91,206.38,8.59">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="26565" to="26577" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,559.80,396.00,8.82;10,117.96,570.76,95.19,8.82" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,290.45,559.98,134.78,8.64">Auto-encoding variational bayes</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,108.00,589.87,396.00,8.64;10,117.96,600.65,386.04,8.82;10,117.96,611.79,22.42,8.64" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,140.27,600.83,221.66,8.64">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName coords=""><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,108.00,630.72,396.00,8.64;10,117.96,641.50,289.82,8.82" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,307.64,630.72,196.36,8.64;10,117.96,641.68,122.79,8.64">Flow straight and fast: Learning to generate and transfer data with rectified flow</title>
		<author>
			<persName coords=""><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.03003</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,108.00,660.61,396.00,8.64;10,117.96,671.39,386.04,8.82;10,117.96,682.53,22.42,8.64" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.06380</idno>
		<title level="m" coord="10,385.35,660.61,118.65,8.64;10,117.96,671.57,233.73,8.64">Instaflow: One step is enough for high-quality diffusion-based text-to-image generation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,108.00,701.46,396.00,8.64;10,117.96,712.24,386.04,8.82;10,117.96,723.20,104.60,8.82" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,440.69,701.46,63.31,8.64;10,117.96,712.42,316.58,8.64">Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps</title>
		<author>
			<persName coords=""><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00927</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,108.00,28.88,32.10,8.64;11,108.00,85.34,396.00,8.64;11,117.96,96.12,386.04,8.82;11,117.96,107.26,27.40,8.64" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="11,422.41,85.34,81.59,8.64;11,117.96,96.30,240.85,8.64">Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models</title>
		<author>
			<persName coords=""><forename type="first">Preprint</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lu</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01095</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,108.00,126.74,396.00,8.64;11,117.96,137.52,315.37,8.82" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="11,383.12,126.74,120.89,8.64;11,117.96,137.70,148.16,8.64">Accelerating diffusion models via early stop of the diffusion process</title>
		<author>
			<persName coords=""><forename type="first">Zhaoyang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xudong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12524</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,108.00,157.18,396.00,8.64;11,117.96,167.96,386.03,8.82;11,117.96,178.92,329.24,8.82" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,184.62,168.14,171.52,8.64">On distillation of guided diffusion models</title>
		<author>
			<persName coords=""><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,380.29,167.96,123.70,8.59;11,117.96,178.92,225.40,8.59">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="14297" to="14306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,198.58,396.00,8.64;11,117.96,209.54,386.04,8.64;11,117.96,220.32,280.38,8.82" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="11,247.17,209.54,256.84,8.64;11,117.96,220.50,112.74,8.64">Glide: Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,108.00,239.98,396.00,8.64;11,117.96,250.76,324.36,8.82" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,301.04,239.98,198.50,8.64">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,128.75,250.76,187.46,8.59">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,270.42,396.00,8.64;11,117.96,281.38,159.90,8.64" xml:id="b17">
	<monogr>
		<ptr target="https://huggingface.co/datasets/Norod78/simpsons-blip-captions" />
		<title level="m" coord="11,108.00,270.42,142.91,8.64">Simpsons blip captions</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Norod78</note>
</biblStruct>

<biblStruct coords="11,108.00,300.86,396.00,8.64;11,117.96,311.82,225.65,8.64" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">M</forename><surname>Justin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pinkney</surname></persName>
		</author>
		<ptr target="https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions/" />
		<title level="m" coord="11,207.33,300.86,92.86,8.64">Pokemon blip captions</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,331.30,396.00,8.64;11,117.96,342.08,348.01,8.82" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<title level="m" coord="11,433.29,331.30,70.72,8.64;11,117.96,342.26,181.28,8.64">Hierarchical textconditional image generation with clip latents</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,108.00,361.74,396.00,8.64;11,117.96,372.52,386.03,8.82;11,117.96,383.48,312.63,8.82" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,480.76,361.74,23.24,8.64;11,117.96,372.70,220.93,8.64">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName coords=""><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,359.75,372.52,144.25,8.59;11,117.96,383.48,208.80,8.59">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,403.14,396.00,8.64;11,117.96,414.10,386.04,8.64;11,117.96,424.88,386.04,8.82;11,117.96,435.84,195.75,8.82" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="11,449.21,414.10,54.79,8.64;11,117.96,425.06,261.03,8.64">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName coords=""><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kamyar</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Burcu</forename><surname>Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,387.24,424.88,116.76,8.59;11,117.96,435.84,94.09,8.59">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="36479" to="36494" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,455.32,396.00,8.82;11,117.96,466.28,134.95,8.82" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="11,237.04,455.50,237.40,8.64">Progressive distillation for fast sampling of diffusion models</title>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.00512</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,108.00,485.94,396.00,8.64;11,117.96,496.90,386.04,8.64;11,117.96,507.68,386.03,8.82;11,117.96,518.64,100.17,8.82" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mehdi</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.08402</idno>
		<title level="m" coord="11,464.71,496.90,39.29,8.64;11,117.96,507.86,312.94,8.64">Laion-5b: An open large-scale dataset for training next generation image-text models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,108.00,538.30,396.00,8.64;11,117.96,549.08,372.56,8.82" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="11,294.96,538.30,209.05,8.64;11,117.96,549.26,117.71,8.64">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName coords=""><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,243.73,549.08,202.83,8.59">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,568.56,396.00,8.82;11,117.96,579.52,139.38,8.82" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="11,324.37,568.74,145.58,8.64">Denoising diffusion implicit models</title>
		<author>
			<persName coords=""><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,108.00,599.18,396.00,8.64;11,117.96,609.96,251.21,8.82" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="11,233.85,599.18,266.45,8.64">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,117.96,609.96,207.11,8.59">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,629.62,396.00,8.64;11,117.96,640.40,386.04,8.82;11,117.96,651.36,105.16,8.82" xml:id="b27">
	<monogr>
		<title level="m" type="main" coord="11,146.63,640.58,292.97,8.64">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,108.00,671.02,396.00,8.64;11,117.96,681.80,386.04,8.82;11,117.96,692.94,22.42,8.64" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="11,350.15,671.02,153.85,8.64;11,117.96,681.98,92.62,8.64">Maximum likelihood training of scorebased diffusion models</title>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Conor</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,222.52,681.80,213.17,8.59">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1415" to="1428" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,712.24,395.99,8.82;11,117.96,723.20,100.17,8.82" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.01469</idno>
		<title level="m" coord="11,361.36,712.42,77.81,8.64">Consistency models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,108.00,28.88,32.10,8.64;12,108.00,85.34,396.00,8.64;12,117.96,96.12,321.48,8.82" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="12,393.40,85.34,110.61,8.64;12,117.96,96.30,153.85,8.64">Learning to efficiently sample from diffusion probabilistic models</title>
		<author>
			<persName coords=""><forename type="first">Preprint</forename><surname>Daniel Watson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03802</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,108.00,115.21,396.00,8.64;12,117.96,125.99,386.04,8.82;12,117.96,136.95,134.95,8.82" xml:id="b31">
	<monogr>
		<title level="m" type="main" coord="12,481.31,115.21,22.69,8.64;12,117.96,126.17,354.91,8.64">Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<author>
			<persName coords=""><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,108.00,156.05,396.00,8.64;12,117.96,166.83,159.58,8.82" xml:id="b32">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Lvmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.05543</idno>
		<title level="m" coord="12,261.31,156.05,238.22,8.64">Adding conditional control to text-to-image diffusion models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,108.00,185.93,396.00,8.64;12,117.96,196.71,386.03,8.82;12,117.96,207.66,170.76,8.82" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="12,487.55,185.93,16.46,8.64;12,117.96,196.88,208.43,8.64">Fast sampling of diffusion models via operator learning</title>
		<author>
			<persName coords=""><forename type="first">Hongkai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weili</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,349.95,196.71,154.05,8.59;12,117.96,207.66,34.69,8.59">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="42390" to="42402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,108.00,226.76,396.00,8.64;12,117.96,237.54,138.93,8.82" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="12,395.69,226.76,108.31,8.64;12,117.96,237.72,56.42,8.64">Truncated diffusion probabilistic models</title>
		<author>
			<persName coords=""><forename type="first">Huangjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,182.44,237.54,13.51,8.59">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
